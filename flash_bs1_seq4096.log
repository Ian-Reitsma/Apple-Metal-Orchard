[orchard][flashpatch] FlashAttention monkeypatch ENABLED (GPT2Attention.forward, log=True, detail=False)
MPS available: True MPS built: True
[orchard] FlashAttention loaded (SHA e4dd5207â€¦)
[orchard][flashpatch] --- FlashAttention Patch Context ---
  PATCH_ENABLED: True
  FLASHATTN_LOG_KERNEL: True
  FLASHATTN_LOG_DETAIL: False
  torch version: 2.7.1
  Model config (first layer): GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.53.0",
  "use_cache": true,
  "vocab_size": 50257
}

  Example Q shape: torch.Size([1, 12, 4096, 64]), dtype: torch.bfloat16, device: mps:0
  ENV: USE_FLASH_ATTN=1, PYTHONHASHSEED=None
-------------------------------------------------------------
[orchard][flashpatch] ERROR: FlashAttention kernel failed on call 5 (layer=4): MPS backend out of memory (MPS allocated: 7.77 GB, other allocations: 780.67 MB, max allowed: 9.07 GB). Tried to allocate 768.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
---- Kernel Input Shapes ----
  Q: torch.Size([1, 12, 4096, 64]), dtype=torch.bfloat16, device=mps:0
  K: torch.Size([1, 12, 4096, 64]), dtype=torch.bfloat16, device=mps:0
  V: torch.Size([1, 12, 4096, 64]), dtype=torch.bfloat16, device=mps:0
[DEBUG][FLASHATTN] Output shape after kernel: torch.Size([1, 12, 4096, 64]), dtype: torch.bfloat16
[DEBUG][AFTER PERMUTE] torch.Size([1, 4096, 12, 64])
[DEBUG][AFTER RESHAPE] torch.Size([1, 4096, 768])
[DEBUG][FLASHATTN] Output shape after kernel: torch.Size([1, 12, 4096, 64]), dtype: torch.bfloat16
[DEBUG][AFTER PERMUTE] torch.Size([1, 4096, 12, 64])
[DEBUG][AFTER RESHAPE] torch.Size([1, 4096, 768])
[DEBUG][FLASHATTN] Output shape after kernel: torch.Size([1, 12, 4096, 64]), dtype: torch.bfloat16
[DEBUG][AFTER PERMUTE] torch.Size([1, 4096, 12, 64])
[DEBUG][AFTER RESHAPE] torch.Size([1, 4096, 768])
[DEBUG][FLASHATTN] Output shape after kernel: torch.Size([1, 12, 4096, 64]), dtype: torch.bfloat16
[DEBUG][AFTER PERMUTE] torch.Size([1, 4096, 12, 64])
[DEBUG][AFTER RESHAPE] torch.Size([1, 4096, 768])
Traceback (most recent call last):
  File "/Users/ianreitsma/projects/orchard/benchmarks/orchard_patch_flash.py", line 94, in flashattention_forward
    attn_output = torch.ops.flash_attn_mps._flash_attn_fwd(
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/_ops.py", line 1158, in __call__
    return self._op(*args, **(kwargs or {}))
RuntimeError: MPS backend out of memory (MPS allocated: 7.77 GB, other allocations: 780.67 MB, max allowed: 9.07 GB). Tried to allocate 768.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/ianreitsma/projects/orchard/benchmarks/orchard_bench_v0.8.py", line 144, in <module>
    model(ids, labels=ids)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1189, in forward
    transformer_outputs = self.transformer(
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/transformers/modeling_layers.py", line 83, in __call__
    return super().__call__(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/orchard-env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/ianreitsma/projects/orchard/benchmarks/orchard_patch_flash.py", line 105, in flashattention_forward
    print(f"[DEBUG][Q] {q.shape} {q.dtype} {q.device}")
NameError: name 'q' is not defined
